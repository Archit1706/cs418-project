{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968a8ae5",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Misinformation Analysis\n",
    "\n",
    "This notebook handles the initial data loading and preprocessing steps for our misinformation analysis task. We will:\n",
    "\n",
    "1. Load YouTube comments from BigQuery\n",
    "2. Download and process FakeNewsNet and LIAR datasets\n",
    "3. Implement multilingual text processing\n",
    "4. Create a balanced training dataset\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ead81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "from googletrans import Translator\n",
    "from deep_translator import MyMemoryTranslator\n",
    "import langdetect\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import zipfile\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "try:\n",
    "    import nest_asyncio  \n",
    "    nest_asyncio.apply()\n",
    "except ImportError:\n",
    "    nest_asyncio = None\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132c143",
   "metadata": {},
   "source": [
    "## 1. Loading YouTube Comments from Local CSVs\n",
    "\n",
    "We'll load the YouTube comments exported in Task 1 from the local CSV files in `../data/` (e.g., `comment1.csv` ... `comment5.csv`). These comments will be used for both training and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91037a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading comment1.csv ...\n",
      "Loading comment2.csv ...\n",
      "Loading comment3.csv ...\n",
      "Loading comment4.csv ...\n",
      "Loading comment5.csv ...\n",
      "Loaded 400000 sampled comments from 5 files (cap ~400000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "\n",
    "# Directory containing comment CSVs (e.g., comment1.csv ... comment5.csv)\n",
    "comments_dir = '../data'\n",
    "comment_files = sorted(glob.glob(os.path.join(comments_dir, 'comment*.csv')))\n",
    "\n",
    "if not comment_files:\n",
    "    raise FileNotFoundError(f\"No comment CSV files found matching 'comment*.csv' in {comments_dir}\")\n",
    "\n",
    "# Diversified sampling across files for speed\n",
    "TOTAL_TARGET_ROWS = 400_000  # adjust for runtime\n",
    "PER_FILE_CAP = max(1, TOTAL_TARGET_ROWS // max(1, len(comment_files)))\n",
    "\n",
    "# Mapped possible column variants to standard names used downstream\n",
    "column_aliases = {\n",
    "    'comment_text': ['comment_text', 'text', 'content', 'body'],\n",
    "    'video_id': ['video_id', 'videoId'],\n",
    "    'comment_id': ['comment_id', 'commentId', 'id'],\n",
    "    'author': ['author', 'author_display_name', 'author_name', 'authorChannelId', 'author_channel_id'],\n",
    "    'published_at': ['published_at', 'publishedAt', 'timestamp', 'created_at', 'createdAt'],\n",
    "    'like_count': ['like_count', 'likeCount', 'likes'],\n",
    "    'reply_count': ['reply_count', 'replyCount', 'replies']\n",
    "}\n",
    "required_cols = ['comment_text', 'video_id', 'comment_id', 'author', 'published_at', 'like_count', 'reply_count']\n",
    "\n",
    "\n",
    "def normalize_comment_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rename_map = {}\n",
    "    for std_col, candidates in column_aliases.items():\n",
    "        for cand in candidates:\n",
    "            if cand in df.columns:\n",
    "                rename_map[cand] = std_col\n",
    "                break\n",
    "    df = df.rename(columns=rename_map)\n",
    "    # Ensured required columns exist\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            if col in ['like_count', 'reply_count']:\n",
    "                df[col] = 0\n",
    "            else:\n",
    "                df[col] = pd.NA\n",
    "    return df[required_cols]\n",
    "\n",
    "\n",
    "parts = []\n",
    "for path in comment_files:\n",
    "    print(f\"Loading {os.path.basename(path)} ...\")\n",
    "    part = pd.read_csv(path, low_memory=False)\n",
    "    part = normalize_comment_columns(part)\n",
    "    # Per-file cap to diversify and reduce runtime\n",
    "    if len(part) > PER_FILE_CAP:\n",
    "        part = part.sample(n=PER_FILE_CAP, random_state=42)\n",
    "    parts.append(part)\n",
    "\n",
    "# Concatenated\n",
    "df_comments = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "\n",
    "if len(df_comments) > TOTAL_TARGET_ROWS:\n",
    "    df_comments = df_comments.sample(n=TOTAL_TARGET_ROWS, random_state=42)\n",
    "\n",
    "\n",
    "df_comments['published_at'] = pd.to_datetime(df_comments['published_at'], errors='coerce')\n",
    "for col in ['like_count', 'reply_count']:\n",
    "    df_comments[col] = pd.to_numeric(df_comments[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "print(f\"Loaded {len(df_comments)} sampled comments from {len(comment_files)} files (cap ~{TOTAL_TARGET_ROWS})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8630c0e",
   "metadata": {},
   "source": [
    "## 2. Download and Process Training Datasets\n",
    "\n",
    "We'll now download and process the FakeNewsNet and LIAR datasets for training our misinformation classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f493b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIAR TSVs loaded: 10200 train, 1262 test, 1283 validation samples\n",
      "Loaded FakeNewsNet CSV: gossipcop_fake.csv with 5323 rows\n",
      "Loaded FakeNewsNet CSV: gossipcop_real.csv with 16817 rows\n",
      "Loaded FakeNewsNet CSV: politifact_fake.csv with 432 rows\n",
      "Loaded FakeNewsNet CSV: politifact_real.csv with 624 rows\n",
      "FakeNewsNet CSV total rows: 23196\n",
      "23196\n"
     ]
    }
   ],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "data_dir = '../data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Loaded LIAR dataset from TSV files in liar_dataset/\n",
    "\n",
    "\n",
    "def load_liar_tsv(filename):\n",
    "    path = os.path.join(data_dir, 'liar_dataset', filename)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Expected LIAR TSV at {path}. Place 'train.tsv', 'test.tsv', and 'valid.tsv' in {os.path.dirname(path)}.\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        df_h = pd.read_csv(path, sep='\\t', header=0, low_memory=False)\n",
    "        if {'label', 'statement'}.issubset(set(df_h.columns)):\n",
    "            df_h = df_h.rename(columns={'statement': 'text'})\n",
    "            return df_h[['text', 'label']].dropna(subset=['text', 'label'])\n",
    "    except Exception:\n",
    "        pass\n",
    "   \n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        on_bad_lines='skip',\n",
    "        engine='python'\n",
    "    )\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(f\"Unexpected LIAR TSV shape in {filename}: need at least 2 columns\")\n",
    "    df = pd.DataFrame({\n",
    "        'text': df.iloc[:, 1].astype(str),\n",
    "        'label': df.iloc[:, 0].astype(str)\n",
    "    })\n",
    "    return df.dropna(subset=['text', 'label'])\n",
    "\n",
    "# Read LIAR TSV splits\n",
    "df_liar_train = load_liar_tsv('train.tsv')\n",
    "df_liar_test = load_liar_tsv('test.tsv')\n",
    "df_liar_val = load_liar_tsv('valid.tsv')\n",
    "\n",
    "print(\n",
    "    f\"LIAR TSVs loaded: {len(df_liar_train)} train, {len(df_liar_test)} test, {len(df_liar_val)} validation samples\"\n",
    ")\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "def load_fakenewsnet_csvs():\n",
    "    base_dir = os.path.join(data_dir, 'FakeNewsNetData')\n",
    "    if not os.path.isdir(base_dir):\n",
    "        print(f\"No FakeNewsNetData directory at {base_dir}\")\n",
    "        return pd.DataFrame(columns=['text', 'label'])\n",
    "    candidates = sorted(glob.glob(os.path.join(base_dir, '*.csv')))\n",
    "    frames = []\n",
    "    for path in candidates:\n",
    "        try:\n",
    "            df = pd.read_csv(path, low_memory=False)\n",
    "           \n",
    "            if 'title' in df.columns:\n",
    "                df['text'] = df['title'].astype(str).str.strip()\n",
    "            elif 'news_url' in df.columns:\n",
    "                df['text'] = df['news_url'].astype(str).str.strip()\n",
    "            # Determined label by filename (fake=1, real=0) if no explicit label column exists\n",
    "            label_col = None\n",
    "            for c in ['label', 'verdict', 'type', 'class', 'target']:\n",
    "                if c in df.columns:\n",
    "                    label_col = c\n",
    "                    break\n",
    "            if label_col is not None:\n",
    "                def map_label(v):\n",
    "                    if isinstance(v, str):\n",
    "                        vs = v.strip().lower()\n",
    "                        if vs in ['fake', 'false', 'pants-fire', 'barely-true']:\n",
    "                            return 1\n",
    "                        if vs in ['real', 'true', 'mostly-true', 'half-true']:\n",
    "                            return 0\n",
    "                    try:\n",
    "                        n = int(v)\n",
    "                        if n in (0, 1):\n",
    "                            return n\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    return None\n",
    "                df['label'] = df[label_col].apply(map_label)\n",
    "            else:\n",
    "                name = os.path.basename(path).lower()\n",
    "                if 'fake' in name:\n",
    "                    df['label'] = 1\n",
    "                elif 'real' in name:\n",
    "                    df['label'] = 0\n",
    "                else:\n",
    "                    df['label'] = None\n",
    "            # Keep only non-empty text rows\n",
    "            if 'text' not in df.columns:\n",
    "                print(f\"Skipping {os.path.basename(path)}: missing both 'title' and 'news_url' to build text\")\n",
    "                continue\n",
    "            df['text'] = df['text'].astype(str).str.strip()\n",
    "            df = df[df['text'] != '']\n",
    "            df = df[['text', 'label']].dropna(subset=['text', 'label'])\n",
    "            frames.append(df)\n",
    "            print(f\"Loaded FakeNewsNet CSV: {os.path.basename(path)} with {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {os.path.basename(path)}: {e}\")\n",
    "    if frames:\n",
    "        return pd.concat(frames, ignore_index=True)\n",
    "    return pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "# Attempt to load FakeNewsNet CSVs\n",
    "df_fnn = load_fakenewsnet_csvs()\n",
    "\n",
    "print(f\"FakeNewsNet CSV total rows: {len(df_fnn)}\")\n",
    "print(len(df_fnn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63e7af",
   "metadata": {},
   "source": [
    "## 3. Multilingual Text Processing\n",
    "\n",
    "Now we'll implement language detection and translation for non-English content. This is crucial since our YouTube comments are in multiple languages (RU/UA/EN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43b9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning dataset...\n",
      "Length of the dataset (after cleaning):  399994 (removed 6 rows)\n",
      "Columns in the dataset:  Index(['comment_text', 'video_id', 'comment_id', 'author', 'published_at', 'like_count', 'reply_count'], dtype='object')\n",
      "Number of missing values in the dataset:  comment_text    0\n",
      "video_id        0\n",
      "comment_id      0\n",
      "author          2\n",
      "published_at    0\n",
      "like_count      0\n",
      "reply_count     0\n",
      "dtype: int64\n",
      "Number of duplicate rows:  0\n",
      "                                        comment_text     video_id                  comment_id           author              published_at  like_count  reply_count\n",
      "0         i meditated to this song and saw my future  zK1mLIeXwsQ  Ugw6Cuof_Y8mvPlKHRV4AaABAg     @kickapoo242 2011-03-24 22:59:00+00:00           0            0\n",
      "1               thorw back fr man when i was like 10  e2oRqyn7ToQ        UghBak-N0jKrS3gCoAEC  @pabloevenor533 2016-12-29 01:16:38+00:00           0            0\n",
      "2  for the life of me i cant understand why the l...  RmvCT7j9MFc  Ugx6IovqUdw6zNGTFX54AaABAg      @wowowo2887 2021-08-23 03:58:38+00:00           0            0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Cleaning dataset...\")\n",
    "\n",
    "start_rows = len(df_comments)\n",
    "\n",
    "# Dropped NAs and whitespace-only comments\n",
    "df_comments['comment_text'] = df_comments['comment_text'].astype(str)\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = html.unescape(s)\n",
    "    s = re.sub(r'<[^>]+>', ' ', s)  # removed HTML tags\n",
    "    s = re.sub(r'http[s]?://\\S+|www\\.\\S+', ' ', s)  # removed URLs\n",
    "    s = re.sub(r'[@#]\\w+', ' ', s)  # mentions/hashtags\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "df_comments['comment_text'] = df_comments['comment_text'].apply(clean_text)\n",
    "\n",
    "\n",
    "\n",
    "# Deduplicated by IDs and text within the same video\n",
    "if 'comment_id' in df_comments.columns:\n",
    "    df_comments = df_comments.drop_duplicates(subset=['comment_id'])\n",
    "\n",
    "\n",
    "\n",
    "# Reported all the stats\n",
    "print(\"Length of the dataset (after cleaning): \", len(df_comments), f\"(removed {start_rows - len(df_comments)} rows)\")\n",
    "print(\"Columns in the dataset: \", df_comments.columns)\n",
    "print(\"Number of missing values in the dataset: \", df_comments.isnull().sum())\n",
    "print(\"Number of duplicate rows: \", df_comments.duplicated().sum())\n",
    "print(df_comments.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb7144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language detection done in 84.2s with 374363 unique strings.\n",
      "Loaded 80326 cached translations.\n",
      "Translating 64515 high-impact unique strings (not in cache)...\n",
      "  Processing batch 1/538 (120 strings)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ym/3gyqlnxn3n3dfnhxnlqfc07h0000gn/T/ipykernel_52412/3779710998.py:151: RuntimeWarning: coroutine 'Translator.translate' was never awaited\n",
      "  translations = translate_batch(batch)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing batch 2/538 (120 strings)...\n",
      "  Processing batch 3/538 (120 strings)...\n",
      "  Processing batch 4/538 (120 strings)...\n",
      "  Processing batch 5/538 (120 strings)...\n",
      "  Processing batch 6/538 (120 strings)...\n",
      "  Processing batch 7/538 (120 strings)...\n",
      "  Processing batch 8/538 (120 strings)...\n",
      "  Processing batch 9/538 (120 strings)...\n",
      "  Processing batch 10/538 (120 strings)...\n",
      "  Processing batch 11/538 (120 strings)...\n",
      "  Processing batch 12/538 (120 strings)...\n",
      "  Processing batch 13/538 (120 strings)...\n",
      "  Processing batch 14/538 (120 strings)...\n",
      "  Processing batch 15/538 (120 strings)...\n",
      "  Processing batch 16/538 (120 strings)...\n",
      "  Processing batch 17/538 (120 strings)...\n",
      "  Processing batch 18/538 (120 strings)...\n",
      "  Processing batch 19/538 (120 strings)...\n",
      "  Processing batch 20/538 (120 strings)...\n",
      "  Processing batch 21/538 (120 strings)...\n",
      "  Processing batch 22/538 (120 strings)...\n",
      "  Processing batch 23/538 (120 strings)...\n",
      "  Processing batch 24/538 (120 strings)...\n",
      "  Processing batch 25/538 (120 strings)...\n",
      "  Processing batch 26/538 (120 strings)...\n",
      "  Processing batch 27/538 (120 strings)...\n",
      "  Processing batch 28/538 (120 strings)...\n",
      "  Processing batch 29/538 (120 strings)...\n",
      "  Processing batch 30/538 (120 strings)...\n",
      "  Processing batch 31/538 (120 strings)...\n",
      "  Processing batch 32/538 (120 strings)...\n",
      "  Processing batch 33/538 (120 strings)...\n",
      "  Processing batch 34/538 (120 strings)...\n",
      "  Processing batch 35/538 (120 strings)...\n",
      "  Processing batch 36/538 (120 strings)...\n",
      "  Processing batch 37/538 (120 strings)...\n",
      "  Processing batch 38/538 (120 strings)...\n",
      "  Processing batch 39/538 (120 strings)...\n",
      "  Processing batch 40/538 (120 strings)...\n",
      "  Processing batch 41/538 (120 strings)...\n",
      "  Processing batch 42/538 (120 strings)...\n",
      "  Processing batch 43/538 (120 strings)...\n",
      "  Processing batch 44/538 (120 strings)...\n",
      "  Processing batch 45/538 (120 strings)...\n",
      "  Processing batch 46/538 (120 strings)...\n",
      "  Processing batch 47/538 (120 strings)...\n",
      "  Processing batch 48/538 (120 strings)...\n",
      "  Processing batch 49/538 (120 strings)...\n",
      "  Processing batch 50/538 (120 strings)...\n",
      "  Processing batch 51/538 (120 strings)...\n",
      "  Processing batch 52/538 (120 strings)...\n",
      "  Processing batch 53/538 (120 strings)...\n",
      "  Processing batch 54/538 (120 strings)...\n",
      "  Processing batch 55/538 (120 strings)...\n",
      "  Processing batch 56/538 (120 strings)...\n",
      "  Processing batch 57/538 (120 strings)...\n",
      "  Processing batch 58/538 (120 strings)...\n",
      "  Processing batch 59/538 (120 strings)...\n",
      "  Processing batch 60/538 (120 strings)...\n",
      "  Processing batch 61/538 (120 strings)...\n",
      "  Processing batch 62/538 (120 strings)...\n",
      "  Processing batch 63/538 (120 strings)...\n",
      "  Processing batch 64/538 (120 strings)...\n",
      "  Processing batch 65/538 (120 strings)...\n",
      "  Processing batch 66/538 (120 strings)...\n",
      "  Processing batch 67/538 (120 strings)...\n",
      "  Processing batch 68/538 (120 strings)...\n",
      "  Processing batch 69/538 (120 strings)...\n",
      "  Processing batch 70/538 (120 strings)...\n",
      "  Processing batch 71/538 (120 strings)...\n",
      "  Processing batch 72/538 (120 strings)...\n",
      "  Processing batch 73/538 (120 strings)...\n",
      "  Processing batch 74/538 (120 strings)...\n",
      "  Processing batch 75/538 (120 strings)...\n",
      "  Processing batch 76/538 (120 strings)...\n",
      "  Processing batch 77/538 (120 strings)...\n",
      "  Processing batch 78/538 (120 strings)...\n",
      "  Processing batch 79/538 (120 strings)...\n",
      "  Processing batch 80/538 (120 strings)...\n",
      "  Processing batch 81/538 (120 strings)...\n",
      "  Processing batch 82/538 (120 strings)...\n",
      "  Processing batch 83/538 (120 strings)...\n",
      "  Processing batch 84/538 (120 strings)...\n",
      "  Processing batch 85/538 (120 strings)...\n",
      "  Processing batch 86/538 (120 strings)...\n",
      "  Processing batch 87/538 (120 strings)...\n",
      "  Processing batch 88/538 (120 strings)...\n",
      "  Processing batch 89/538 (120 strings)...\n",
      "  Processing batch 90/538 (120 strings)...\n",
      "  Processing batch 91/538 (120 strings)...\n",
      "  Processing batch 92/538 (120 strings)...\n",
      "  Processing batch 93/538 (120 strings)...\n",
      "  Processing batch 94/538 (120 strings)...\n",
      "  Processing batch 95/538 (120 strings)...\n",
      "  Processing batch 96/538 (120 strings)...\n",
      "  Processing batch 97/538 (120 strings)...\n",
      "  Processing batch 98/538 (120 strings)...\n",
      "  Processing batch 99/538 (120 strings)...\n",
      "  Processing batch 100/538 (120 strings)...\n",
      "  Processing batch 101/538 (120 strings)...\n",
      "  Processing batch 102/538 (120 strings)...\n",
      "  Processing batch 103/538 (120 strings)...\n",
      "  Processing batch 104/538 (120 strings)...\n",
      "  Processing batch 105/538 (120 strings)...\n",
      "  Processing batch 106/538 (120 strings)...\n",
      "  Processing batch 107/538 (120 strings)...\n",
      "  Processing batch 108/538 (120 strings)...\n",
      "  Processing batch 109/538 (120 strings)...\n",
      "  Processing batch 110/538 (120 strings)...\n",
      "  Processing batch 111/538 (120 strings)...\n",
      "  Processing batch 112/538 (120 strings)...\n",
      "  Processing batch 113/538 (120 strings)...\n",
      "  Processing batch 114/538 (120 strings)...\n",
      "  Processing batch 115/538 (120 strings)...\n",
      "  Processing batch 116/538 (120 strings)...\n",
      "  Processing batch 117/538 (120 strings)...\n",
      "  Processing batch 118/538 (120 strings)...\n",
      "  Processing batch 119/538 (120 strings)...\n",
      "  Processing batch 120/538 (120 strings)...\n",
      "  Processing batch 121/538 (120 strings)...\n",
      "  Processing batch 122/538 (120 strings)...\n",
      "  Processing batch 123/538 (120 strings)...\n",
      "  Processing batch 124/538 (120 strings)...\n",
      "  Processing batch 125/538 (120 strings)...\n",
      "  Processing batch 126/538 (120 strings)...\n",
      "  Processing batch 127/538 (120 strings)...\n",
      "  Processing batch 128/538 (120 strings)...\n",
      "  Processing batch 129/538 (120 strings)...\n",
      "  Processing batch 130/538 (120 strings)...\n",
      "  Processing batch 131/538 (120 strings)...\n",
      "  Processing batch 132/538 (120 strings)...\n",
      "  Processing batch 133/538 (120 strings)...\n",
      "  Processing batch 134/538 (120 strings)...\n",
      "  Processing batch 135/538 (120 strings)...\n",
      "  Processing batch 136/538 (120 strings)...\n",
      "  Processing batch 137/538 (120 strings)...\n",
      "  Processing batch 138/538 (120 strings)...\n",
      "  Processing batch 139/538 (120 strings)...\n",
      "  Processing batch 140/538 (120 strings)...\n",
      "  Processing batch 141/538 (120 strings)...\n",
      "  Processing batch 142/538 (120 strings)...\n",
      "  Processing batch 143/538 (120 strings)...\n",
      "  Processing batch 144/538 (120 strings)...\n",
      "  Processing batch 145/538 (120 strings)...\n",
      "  Processing batch 146/538 (120 strings)...\n",
      "  Processing batch 147/538 (120 strings)...\n",
      "  Processing batch 148/538 (120 strings)...\n",
      "  Processing batch 149/538 (120 strings)...\n",
      "  Processing batch 150/538 (120 strings)...\n",
      "  Processing batch 151/538 (120 strings)...\n",
      "  Processing batch 152/538 (120 strings)...\n",
      "  Processing batch 153/538 (120 strings)...\n",
      "  Processing batch 154/538 (120 strings)...\n",
      "  Processing batch 155/538 (120 strings)...\n",
      "  Processing batch 156/538 (120 strings)...\n",
      "  Processing batch 157/538 (120 strings)...\n",
      "  Processing batch 158/538 (120 strings)...\n",
      "  Processing batch 159/538 (120 strings)...\n",
      "  Processing batch 160/538 (120 strings)...\n",
      "  Processing batch 161/538 (120 strings)...\n",
      "  Processing batch 162/538 (120 strings)...\n",
      "  Processing batch 163/538 (120 strings)...\n",
      "  Processing batch 164/538 (120 strings)...\n",
      "  Processing batch 165/538 (120 strings)...\n",
      "  Processing batch 166/538 (120 strings)...\n",
      "  Processing batch 167/538 (120 strings)...\n",
      "  Processing batch 168/538 (120 strings)...\n",
      "  Processing batch 169/538 (120 strings)...\n",
      "  Processing batch 170/538 (120 strings)...\n",
      "  Processing batch 171/538 (120 strings)...\n",
      "  Processing batch 172/538 (120 strings)...\n",
      "  Processing batch 173/538 (120 strings)...\n",
      "  Processing batch 174/538 (120 strings)...\n",
      "  Processing batch 175/538 (120 strings)...\n",
      "  Processing batch 176/538 (120 strings)...\n",
      "  Processing batch 177/538 (120 strings)...\n",
      "  Processing batch 178/538 (120 strings)...\n",
      "  Processing batch 179/538 (120 strings)...\n",
      "  Processing batch 180/538 (120 strings)...\n",
      "  Processing batch 181/538 (120 strings)...\n",
      "  Processing batch 182/538 (120 strings)...\n",
      "  Processing batch 183/538 (120 strings)...\n",
      "  Processing batch 184/538 (120 strings)...\n",
      "  Processing batch 185/538 (120 strings)...\n",
      "  Processing batch 186/538 (120 strings)...\n",
      "  Processing batch 187/538 (120 strings)...\n",
      "  Processing batch 188/538 (120 strings)...\n",
      "  Processing batch 189/538 (120 strings)...\n",
      "  Processing batch 190/538 (120 strings)...\n",
      "  Processing batch 191/538 (120 strings)...\n",
      "  Processing batch 192/538 (120 strings)...\n",
      "  Processing batch 193/538 (120 strings)...\n",
      "  Processing batch 194/538 (120 strings)...\n",
      "  Processing batch 195/538 (120 strings)...\n",
      "  Processing batch 196/538 (120 strings)...\n",
      "  Processing batch 197/538 (120 strings)...\n",
      "  Processing batch 198/538 (120 strings)...\n",
      "  Processing batch 199/538 (120 strings)...\n",
      "  Processing batch 200/538 (120 strings)...\n",
      "  Processing batch 201/538 (120 strings)...\n",
      "  Processing batch 202/538 (120 strings)...\n",
      "  Processing batch 203/538 (120 strings)...\n",
      "  Processing batch 204/538 (120 strings)...\n",
      "  Processing batch 205/538 (120 strings)...\n",
      "  Processing batch 206/538 (120 strings)...\n",
      "  Processing batch 207/538 (120 strings)...\n",
      "  Processing batch 208/538 (120 strings)...\n",
      "  Processing batch 209/538 (120 strings)...\n",
      "  Processing batch 210/538 (120 strings)...\n",
      "  Processing batch 211/538 (120 strings)...\n",
      "  Processing batch 212/538 (120 strings)...\n",
      "  Processing batch 213/538 (120 strings)...\n",
      "  Processing batch 214/538 (120 strings)...\n",
      "  Processing batch 215/538 (120 strings)...\n",
      "  Processing batch 216/538 (120 strings)...\n",
      "  Processing batch 217/538 (120 strings)...\n",
      "  Processing batch 218/538 (120 strings)...\n",
      "  Processing batch 219/538 (120 strings)...\n",
      "  Processing batch 220/538 (120 strings)...\n",
      "  Processing batch 221/538 (120 strings)...\n",
      "  Processing batch 222/538 (120 strings)...\n",
      "  Processing batch 223/538 (120 strings)...\n",
      "  Processing batch 224/538 (120 strings)...\n",
      "  Processing batch 225/538 (120 strings)...\n",
      "  Processing batch 226/538 (120 strings)...\n",
      "  Processing batch 227/538 (120 strings)...\n",
      "  Processing batch 228/538 (120 strings)...\n",
      "  Processing batch 229/538 (120 strings)...\n",
      "  Processing batch 230/538 (120 strings)...\n",
      "  Processing batch 231/538 (120 strings)...\n",
      "  Processing batch 232/538 (120 strings)...\n",
      "  Processing batch 233/538 (120 strings)...\n",
      "  Processing batch 234/538 (120 strings)...\n",
      "  Processing batch 235/538 (120 strings)...\n",
      "  Processing batch 236/538 (120 strings)...\n",
      "  Processing batch 237/538 (120 strings)...\n",
      "  Processing batch 238/538 (120 strings)...\n",
      "  Processing batch 239/538 (120 strings)...\n",
      "  Processing batch 240/538 (120 strings)...\n",
      "  Processing batch 241/538 (120 strings)...\n",
      "  Processing batch 242/538 (120 strings)...\n",
      "  Processing batch 243/538 (120 strings)...\n",
      "  Processing batch 244/538 (120 strings)...\n",
      "  Processing batch 245/538 (120 strings)...\n",
      "  Processing batch 246/538 (120 strings)...\n",
      "  Processing batch 247/538 (120 strings)...\n",
      "  Processing batch 248/538 (120 strings)...\n",
      "  Processing batch 249/538 (120 strings)...\n",
      "  Processing batch 250/538 (120 strings)...\n",
      "  Processing batch 251/538 (120 strings)...\n",
      "  Processing batch 252/538 (120 strings)...\n",
      "  Processing batch 253/538 (120 strings)...\n",
      "  Processing batch 254/538 (120 strings)...\n",
      "  Processing batch 255/538 (120 strings)...\n",
      "  Processing batch 256/538 (120 strings)...\n",
      "  Processing batch 257/538 (120 strings)...\n",
      "  Processing batch 258/538 (120 strings)...\n",
      "  Processing batch 259/538 (120 strings)...\n",
      "  Processing batch 260/538 (120 strings)...\n",
      "  Processing batch 261/538 (120 strings)...\n",
      "  Processing batch 262/538 (120 strings)...\n",
      "  Processing batch 263/538 (120 strings)...\n",
      "  Processing batch 264/538 (120 strings)...\n",
      "  Processing batch 265/538 (120 strings)...\n",
      "  Processing batch 266/538 (120 strings)...\n",
      "  Processing batch 267/538 (120 strings)...\n",
      "  Processing batch 268/538 (120 strings)...\n",
      "  Processing batch 269/538 (120 strings)...\n",
      "  Processing batch 270/538 (120 strings)...\n",
      "  Processing batch 271/538 (120 strings)...\n",
      "  Processing batch 272/538 (120 strings)...\n",
      "  Processing batch 273/538 (120 strings)...\n",
      "  Processing batch 274/538 (120 strings)...\n",
      "  Processing batch 275/538 (120 strings)...\n",
      "  Processing batch 276/538 (120 strings)...\n",
      "  Processing batch 277/538 (120 strings)...\n",
      "  Processing batch 278/538 (120 strings)...\n",
      "  Processing batch 279/538 (120 strings)...\n",
      "  Processing batch 280/538 (120 strings)...\n",
      "  Processing batch 281/538 (120 strings)...\n",
      "  Processing batch 282/538 (120 strings)...\n",
      "  Processing batch 283/538 (120 strings)...\n",
      "  Processing batch 284/538 (120 strings)...\n",
      "  Processing batch 285/538 (120 strings)...\n",
      "  Processing batch 286/538 (120 strings)...\n",
      "  Processing batch 287/538 (120 strings)...\n",
      "  Processing batch 288/538 (120 strings)...\n",
      "  Processing batch 289/538 (120 strings)...\n",
      "  Processing batch 290/538 (120 strings)...\n",
      "  Processing batch 291/538 (120 strings)...\n",
      "  Processing batch 292/538 (120 strings)...\n",
      "  Processing batch 293/538 (120 strings)...\n",
      "  Processing batch 294/538 (120 strings)...\n",
      "  Processing batch 295/538 (120 strings)...\n",
      "  Processing batch 296/538 (120 strings)...\n",
      "  Processing batch 297/538 (120 strings)...\n",
      "  Processing batch 298/538 (120 strings)...\n",
      "  Processing batch 299/538 (120 strings)...\n",
      "  Processing batch 300/538 (120 strings)...\n",
      "  Processing batch 301/538 (120 strings)...\n",
      "  Processing batch 302/538 (120 strings)...\n",
      "  Processing batch 303/538 (120 strings)...\n",
      "  Processing batch 304/538 (120 strings)...\n",
      "  Processing batch 305/538 (120 strings)...\n",
      "  Processing batch 306/538 (120 strings)...\n",
      "  Processing batch 307/538 (120 strings)...\n",
      "  Processing batch 308/538 (120 strings)...\n",
      "  Processing batch 309/538 (120 strings)...\n",
      "  Processing batch 310/538 (120 strings)...\n",
      "  Processing batch 311/538 (120 strings)...\n",
      "  Processing batch 312/538 (120 strings)...\n",
      "  Processing batch 313/538 (120 strings)...\n",
      "  Processing batch 314/538 (120 strings)...\n",
      "  Processing batch 315/538 (120 strings)...\n",
      "  Processing batch 316/538 (120 strings)...\n",
      "  Processing batch 317/538 (120 strings)...\n",
      "  Processing batch 318/538 (120 strings)...\n",
      "  Processing batch 319/538 (120 strings)...\n",
      "  Processing batch 320/538 (120 strings)...\n",
      "  Processing batch 321/538 (120 strings)...\n",
      "  Processing batch 322/538 (120 strings)...\n",
      "  Processing batch 323/538 (120 strings)...\n",
      "  Processing batch 324/538 (120 strings)...\n",
      "  Processing batch 325/538 (120 strings)...\n",
      "  Processing batch 326/538 (120 strings)...\n",
      "  Processing batch 327/538 (120 strings)...\n",
      "  Processing batch 328/538 (120 strings)...\n",
      "  Processing batch 329/538 (120 strings)...\n",
      "  Processing batch 330/538 (120 strings)...\n",
      "  Processing batch 331/538 (120 strings)...\n",
      "  Processing batch 332/538 (120 strings)...\n",
      "  Processing batch 333/538 (120 strings)...\n",
      "  Processing batch 334/538 (120 strings)...\n",
      "  Processing batch 335/538 (120 strings)...\n",
      "  Processing batch 336/538 (120 strings)...\n",
      "  Processing batch 337/538 (120 strings)...\n",
      "  Processing batch 338/538 (120 strings)...\n",
      "  Processing batch 339/538 (120 strings)...\n",
      "  Processing batch 340/538 (120 strings)...\n",
      "  Processing batch 341/538 (120 strings)...\n",
      "  Processing batch 342/538 (120 strings)...\n",
      "  Processing batch 343/538 (120 strings)...\n",
      "  Processing batch 344/538 (120 strings)...\n",
      "  Processing batch 345/538 (120 strings)...\n",
      "  Processing batch 346/538 (120 strings)...\n",
      "  Processing batch 347/538 (120 strings)...\n",
      "  Processing batch 348/538 (120 strings)...\n",
      "  Processing batch 349/538 (120 strings)...\n",
      "  Processing batch 350/538 (120 strings)...\n",
      "  Processing batch 351/538 (120 strings)...\n",
      "  Processing batch 352/538 (120 strings)...\n",
      "  Processing batch 353/538 (120 strings)...\n",
      "  Processing batch 354/538 (120 strings)...\n",
      "  Processing batch 355/538 (120 strings)...\n",
      "  Processing batch 356/538 (120 strings)...\n",
      "  Processing batch 357/538 (120 strings)...\n",
      "  Processing batch 358/538 (120 strings)...\n",
      "  Processing batch 359/538 (120 strings)...\n",
      "  Processing batch 360/538 (120 strings)...\n",
      "  Processing batch 361/538 (120 strings)...\n",
      "  Processing batch 362/538 (120 strings)...\n",
      "  Processing batch 363/538 (120 strings)...\n",
      "  Processing batch 364/538 (120 strings)...\n",
      "  Processing batch 365/538 (120 strings)...\n",
      "  Processing batch 366/538 (120 strings)...\n",
      "  Processing batch 367/538 (120 strings)...\n",
      "  Processing batch 368/538 (120 strings)...\n",
      "  Processing batch 369/538 (120 strings)...\n",
      "  Processing batch 370/538 (120 strings)...\n",
      "  Processing batch 371/538 (120 strings)...\n",
      "  Processing batch 372/538 (120 strings)...\n",
      "  Processing batch 373/538 (120 strings)...\n",
      "  Processing batch 374/538 (120 strings)...\n",
      "  Processing batch 375/538 (120 strings)...\n",
      "  Processing batch 376/538 (120 strings)...\n",
      "  Processing batch 377/538 (120 strings)...\n",
      "  Processing batch 378/538 (120 strings)...\n",
      "  Processing batch 379/538 (120 strings)...\n",
      "  Processing batch 380/538 (120 strings)...\n",
      "  Processing batch 381/538 (120 strings)...\n",
      "  Processing batch 382/538 (120 strings)...\n",
      "  Processing batch 383/538 (120 strings)...\n",
      "  Processing batch 384/538 (120 strings)...\n",
      "  Processing batch 385/538 (120 strings)...\n",
      "  Processing batch 386/538 (120 strings)...\n",
      "  Processing batch 387/538 (120 strings)...\n",
      "  Processing batch 388/538 (120 strings)...\n",
      "  Processing batch 389/538 (120 strings)...\n",
      "  Processing batch 390/538 (120 strings)...\n",
      "  Processing batch 391/538 (120 strings)...\n",
      "  Processing batch 392/538 (120 strings)...\n",
      "  Processing batch 393/538 (120 strings)...\n",
      "  Processing batch 394/538 (120 strings)...\n",
      "  Processing batch 395/538 (120 strings)...\n",
      "  Processing batch 396/538 (120 strings)...\n",
      "  Processing batch 397/538 (120 strings)...\n",
      "  Processing batch 398/538 (120 strings)...\n",
      "  Processing batch 399/538 (120 strings)...\n",
      "  Processing batch 400/538 (120 strings)...\n",
      "  Processing batch 401/538 (120 strings)...\n",
      "  Processing batch 402/538 (120 strings)...\n",
      "  Processing batch 403/538 (120 strings)...\n",
      "  Processing batch 404/538 (120 strings)...\n",
      "  Processing batch 405/538 (120 strings)...\n",
      "  Processing batch 406/538 (120 strings)...\n",
      "  Processing batch 407/538 (120 strings)...\n",
      "  Processing batch 408/538 (120 strings)...\n",
      "  Processing batch 409/538 (120 strings)...\n",
      "  Processing batch 410/538 (120 strings)...\n",
      "  Processing batch 411/538 (120 strings)...\n",
      "  Processing batch 412/538 (120 strings)...\n",
      "  Processing batch 413/538 (120 strings)...\n",
      "  Processing batch 414/538 (120 strings)...\n",
      "  Processing batch 415/538 (120 strings)...\n",
      "  Processing batch 416/538 (120 strings)...\n",
      "  Processing batch 417/538 (120 strings)...\n",
      "  Processing batch 418/538 (120 strings)...\n",
      "  Processing batch 419/538 (120 strings)...\n",
      "  Processing batch 420/538 (120 strings)...\n",
      "  Processing batch 421/538 (120 strings)...\n",
      "  Processing batch 422/538 (120 strings)...\n",
      "  Processing batch 423/538 (120 strings)...\n",
      "  Processing batch 424/538 (120 strings)...\n",
      "  Processing batch 425/538 (120 strings)...\n",
      "  Processing batch 426/538 (120 strings)...\n",
      "  Processing batch 427/538 (120 strings)...\n",
      "  Processing batch 428/538 (120 strings)...\n",
      "  Processing batch 429/538 (120 strings)...\n",
      "  Processing batch 430/538 (120 strings)...\n",
      "  Processing batch 431/538 (120 strings)...\n",
      "  Processing batch 432/538 (120 strings)...\n",
      "  Processing batch 433/538 (120 strings)...\n",
      "  Processing batch 434/538 (120 strings)...\n",
      "  Processing batch 435/538 (120 strings)...\n",
      "  Processing batch 436/538 (120 strings)...\n",
      "  Processing batch 437/538 (120 strings)...\n",
      "  Processing batch 438/538 (120 strings)...\n",
      "  Processing batch 439/538 (120 strings)...\n",
      "  Processing batch 440/538 (120 strings)...\n",
      "  Processing batch 441/538 (120 strings)...\n",
      "  Processing batch 442/538 (120 strings)...\n",
      "  Processing batch 443/538 (120 strings)...\n",
      "  Processing batch 444/538 (120 strings)...\n",
      "  Processing batch 445/538 (120 strings)...\n",
      "  Processing batch 446/538 (120 strings)...\n",
      "  Processing batch 447/538 (120 strings)...\n",
      "  Processing batch 448/538 (120 strings)...\n",
      "  Processing batch 449/538 (120 strings)...\n",
      "  Processing batch 450/538 (120 strings)...\n",
      "  Processing batch 451/538 (120 strings)...\n",
      "  Processing batch 452/538 (120 strings)...\n",
      "  Processing batch 453/538 (120 strings)...\n",
      "  Processing batch 454/538 (120 strings)...\n",
      "  Processing batch 455/538 (120 strings)...\n",
      "  Processing batch 456/538 (120 strings)...\n",
      "  Processing batch 457/538 (120 strings)...\n",
      "  Processing batch 458/538 (120 strings)...\n",
      "  Processing batch 459/538 (120 strings)...\n",
      "  Processing batch 460/538 (120 strings)...\n",
      "  Processing batch 461/538 (120 strings)...\n",
      "  Processing batch 462/538 (120 strings)...\n",
      "  Processing batch 463/538 (120 strings)...\n",
      "  Processing batch 464/538 (120 strings)...\n",
      "  Processing batch 465/538 (120 strings)...\n",
      "  Processing batch 466/538 (120 strings)...\n",
      "  Processing batch 467/538 (120 strings)...\n",
      "  Processing batch 468/538 (120 strings)...\n",
      "  Processing batch 469/538 (120 strings)...\n",
      "  Processing batch 470/538 (120 strings)...\n",
      "  Processing batch 471/538 (120 strings)...\n",
      "  Processing batch 472/538 (120 strings)...\n",
      "  Processing batch 473/538 (120 strings)...\n",
      "  Processing batch 474/538 (120 strings)...\n",
      "  Processing batch 475/538 (120 strings)...\n",
      "  Processing batch 476/538 (120 strings)...\n",
      "  Processing batch 477/538 (120 strings)...\n",
      "  Processing batch 478/538 (120 strings)...\n",
      "  Processing batch 479/538 (120 strings)...\n",
      "  Processing batch 480/538 (120 strings)...\n",
      "  Processing batch 481/538 (120 strings)...\n",
      "  Processing batch 482/538 (120 strings)...\n",
      "  Processing batch 483/538 (120 strings)...\n",
      "  Processing batch 484/538 (120 strings)...\n",
      "  Processing batch 485/538 (120 strings)...\n",
      "  Processing batch 486/538 (120 strings)...\n",
      "  Processing batch 487/538 (120 strings)...\n",
      "  Processing batch 488/538 (120 strings)...\n",
      "  Processing batch 489/538 (120 strings)...\n",
      "  Processing batch 490/538 (120 strings)...\n",
      "  Processing batch 491/538 (120 strings)...\n",
      "  Processing batch 492/538 (120 strings)...\n",
      "  Processing batch 493/538 (120 strings)...\n",
      "  Processing batch 494/538 (120 strings)...\n",
      "  Processing batch 495/538 (120 strings)...\n",
      "  Processing batch 496/538 (120 strings)...\n",
      "  Processing batch 497/538 (120 strings)...\n",
      "  Processing batch 498/538 (120 strings)...\n",
      "  Processing batch 499/538 (120 strings)...\n",
      "  Processing batch 500/538 (120 strings)...\n",
      "  Processing batch 501/538 (120 strings)...\n",
      "  Processing batch 502/538 (120 strings)...\n",
      "  Processing batch 503/538 (120 strings)...\n",
      "  Processing batch 504/538 (120 strings)...\n",
      "  Processing batch 505/538 (120 strings)...\n",
      "  Processing batch 506/538 (120 strings)...\n",
      "  Processing batch 507/538 (120 strings)...\n",
      "  Processing batch 508/538 (120 strings)...\n",
      "  Processing batch 509/538 (120 strings)...\n",
      "  Processing batch 510/538 (120 strings)...\n",
      "  Processing batch 511/538 (120 strings)...\n",
      "  Processing batch 512/538 (120 strings)...\n",
      "  Processing batch 513/538 (120 strings)...\n",
      "  Processing batch 514/538 (120 strings)...\n",
      "  Processing batch 515/538 (120 strings)...\n",
      "  Processing batch 516/538 (120 strings)...\n",
      "  Processing batch 517/538 (120 strings)...\n",
      "  Processing batch 518/538 (120 strings)...\n",
      "  Processing batch 519/538 (120 strings)...\n",
      "  Processing batch 520/538 (120 strings)...\n",
      "  Processing batch 521/538 (120 strings)...\n",
      "  Processing batch 522/538 (120 strings)...\n",
      "  Processing batch 523/538 (120 strings)...\n",
      "  Processing batch 524/538 (120 strings)...\n",
      "  Processing batch 525/538 (120 strings)...\n",
      "  Processing batch 526/538 (120 strings)...\n",
      "  Processing batch 527/538 (120 strings)...\n",
      "  Processing batch 528/538 (120 strings)...\n",
      "  Processing batch 529/538 (120 strings)...\n",
      "  Processing batch 530/538 (120 strings)...\n",
      "  Processing batch 531/538 (120 strings)...\n",
      "  Processing batch 532/538 (120 strings)...\n",
      "  Processing batch 533/538 (120 strings)...\n",
      "  Processing batch 534/538 (120 strings)...\n",
      "  Processing batch 535/538 (120 strings)...\n",
      "  Processing batch 536/538 (120 strings)...\n",
      "  Processing batch 537/538 (120 strings)...\n",
      "  Processing batch 538/538 (75 strings)...\n",
      "Translated 538 unique strings in 137.3s (3.9 strings/s)\n",
      "Estimated coverage of non-English rows this run: 0.6%\n",
      "  Note: not all non-English strings were translated this run. Re-run later to continue using the cache.\n",
      "\n",
      "=== Translation Summary ===\n",
      "Non-English rows: 85564\n",
      "Translated fraction: 0.110\n",
      "Total cache size: 80864 translations\n",
      "Cell runtime: 222.3s (target < 300s)\n",
      "Remaining non-English rows will translate on the next run; cached results prevent duplicates.\n",
      "\n",
      "Language distribution:\n",
      "detected_lang\n",
      "en    314430\n",
      "ja     12937\n",
      "zh      8259\n",
      "ar      7586\n",
      "es      7367\n",
      "ru      5779\n",
      "pt      4609\n",
      "ko      3045\n",
      "fr      2751\n",
      "tr      2594\n",
      "la      2358\n",
      "hi      1238\n",
      "km      1164\n",
      "mr      1153\n",
      "fa       993\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fast multilingual processing with translation enabled\n",
    "# 1) Quick language detection (ASCII heuristic + optional pycld3 + fallback)\n",
    "# 2) Full translation of all non-English text with caching and fallback\n",
    "# 3) Persistent cache to disk for reusability; chunked processing for robustness\n",
    "\n",
    "import time\n",
    "\n",
    "# ---- Tunables ----\n",
    "ASCII_EN_RATIO = 0.98               # pre-label EN if >= 98% ASCII\n",
    "DETECT_MAX_WORKERS = min(12, max(2, (os.cpu_count() or 8) - 1))\n",
    "DO_TRANSLATE = True                 # Enable translation for non-English text\n",
    "TRANSLATE_MAX_UNIQUE = None         # Process as many unique strings as budgets allow (ordered by frequency)\n",
    "TRANSLATE_TIME_BUDGET_S = 240       # Seconds dedicated to translation within this run (~4 min)\n",
    "OVERALL_TIME_BUDGET_S = 300         # Upper bound for entire cell (~5 min)\n",
    "BATCH_SIZE = 120                    # Number of strings sent per request\n",
    "MAX_RETRIES = 3                     # Retry budget per batch\n",
    "PAUSE_BETWEEN_BATCHES = 0.25        # Cooldown between requests (seconds)\n",
    "USE_FALLBACK_TRANSLATOR = False     # Optional secondary provider (disabled by default)\n",
    "cache_path = os.path.join(data_dir, 'translation_cache.jsonl')\n",
    "\n",
    "\n",
    "overall_start = time.time()\n",
    "\n",
    "# Tried with ultra-fast detector\n",
    "HAVE_CLD3 = False\n",
    "try:\n",
    "    import pycld3\n",
    "    HAVE_CLD3 = True\n",
    "except Exception:\n",
    "    HAVE_CLD3 = False\n",
    "\n",
    "\n",
    "def ascii_ratio(s: str) -> float:\n",
    "    if not s:\n",
    "        return 1.0\n",
    "    return sum(ch.isascii() for ch in s) / len(s)\n",
    "\n",
    "\n",
    "def detect_lang_one(text: str) -> str:\n",
    "   \n",
    "    if ascii_ratio(text) >= ASCII_EN_RATIO:\n",
    "        return 'en'\n",
    "    if HAVE_CLD3:\n",
    "        try:\n",
    "            r = pycld3.get_language(text)\n",
    "            if r and r.is_reliable and r.language:\n",
    "                return r.language\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Fallbacks: langid -> langdetect\n",
    "    try:\n",
    "        import langid  # noqa: F401\n",
    "        code, _ = langid.classify(text)\n",
    "        return code\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return langdetect.detect(text)\n",
    "    except Exception:\n",
    "        return 'unknown'\n",
    "\n",
    "# Deduplicated for detection\n",
    "texts = df_comments['comment_text'].astype(str).fillna('')\n",
    "unique_texts = texts.drop_duplicates()\n",
    "\n",
    "start_det = time.time()\n",
    "# Threaded detection to avoid pickling issues in notebooks\n",
    "with ThreadPoolExecutor(max_workers=DETECT_MAX_WORKERS) as pool:\n",
    "    unique_langs = list(pool.map(detect_lang_one, unique_texts.tolist()))\n",
    "lang_map = pd.Series(unique_langs, index=unique_texts.values)\n",
    "df_comments['detected_lang'] = texts.map(lang_map).fillna('unknown')\n",
    "det_time = time.time() - start_det\n",
    "print(f\"Language detection done in {det_time:.1f}s with {len(unique_texts)} unique strings.\")\n",
    "\n",
    "# Built/loaded translation cache (text -> translation)\n",
    "translation_cache: dict[str, str] = {}\n",
    "if os.path.exists(cache_path):\n",
    "    try:\n",
    "        with open(cache_path, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    s = obj.get('text'); t = obj.get('translation')\n",
    "                    if s is not None and t is not None:\n",
    "                        translation_cache[s] = t\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"Loaded {len(translation_cache)} cached translations.\")\n",
    "\n",
    "\n",
    "df_comments['translated_text'] = df_comments['comment_text']\n",
    "\n",
    "if DO_TRANSLATE:\n",
    "    # Translate non-English texts prioritising the most frequent strings first\n",
    "    non_en_mask = (df_comments['detected_lang'] != 'en') & (df_comments['detected_lang'] != 'unknown')\n",
    "    total_non_en_rows = int(non_en_mask.sum())\n",
    "    non_en_series = df_comments.loc[non_en_mask, 'comment_text'].astype(str)\n",
    "    freq_series = non_en_series.value_counts()\n",
    "\n",
    "    candidates = [text for text in freq_series.index if text and text not in translation_cache]\n",
    "    if TRANSLATE_MAX_UNIQUE is not None:\n",
    "        candidates = candidates[:TRANSLATE_MAX_UNIQUE]\n",
    "\n",
    "    if OVERALL_TIME_BUDGET_S is not None and (time.time() - overall_start) >= OVERALL_TIME_BUDGET_S:\n",
    "        print(\"Overall time budget exhausted before translation step; skipping translation.\")\n",
    "        candidates = []\n",
    "\n",
    "    print(f\"Translating {len(candidates)} high-impact unique strings (not in cache)...\")\n",
    "    start_tr = time.time()\n",
    "    translated_unique = 0\n",
    "    translated_rows = 0\n",
    "\n",
    "    translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "    fallback_translator = None\n",
    "    if USE_FALLBACK_TRANSLATOR:\n",
    "        try:\n",
    "            fallback_translator = MyMemoryTranslator(source='en', target='en')\n",
    "        except Exception as exc:\n",
    "            fallback_translator = None\n",
    "            print(f\"  Fallback translator unavailable ({exc}); continuing without it.\")\n",
    "\n",
    "    def translate_batch(batch: list[str]) -> list[str]:\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                result = translator.translate(batch, dest='en')\n",
    "                if not isinstance(result, list):\n",
    "                    result = [result]\n",
    "                return [getattr(item, 'text', str(item)) for item in result]\n",
    "            except Exception:\n",
    "                wait_time = PAUSE_BETWEEN_BATCHES * attempt\n",
    "                time.sleep(wait_time)\n",
    "        return [batch_item for batch_item in batch]\n",
    "\n",
    "    batches = [candidates[i:i + BATCH_SIZE] for i in range(0, len(candidates), BATCH_SIZE)]\n",
    "    failed_candidates = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(batches, start=1):\n",
    "        elapsed_overall = time.time() - overall_start\n",
    "        elapsed_trans = time.time() - start_tr\n",
    "        if OVERALL_TIME_BUDGET_S is not None and elapsed_overall >= OVERALL_TIME_BUDGET_S:\n",
    "            print(\"  Overall time budget reached; stopping translation.\")\n",
    "            break\n",
    "        if TRANSLATE_TIME_BUDGET_S is not None and elapsed_trans >= TRANSLATE_TIME_BUDGET_S:\n",
    "            print(f\"  Translation time budget ({TRANSLATE_TIME_BUDGET_S}s) reached; stopping translation.\")\n",
    "            break\n",
    "\n",
    "        print(f\"  Processing batch {batch_idx}/{len(batches)} ({len(batch)} strings)...\")\n",
    "        translations = translate_batch(batch)\n",
    "\n",
    "        successes: list[tuple[str, str]] = []\n",
    "        for original, translated in zip(batch, translations):\n",
    "            original_clean = original.strip()\n",
    "            translated_clean = (translated or '').strip()\n",
    "\n",
    "            if not translated_clean or translated_clean.lower() == original_clean.lower():\n",
    "                if fallback_translator is not None:\n",
    "                    try:\n",
    "                        fallback = (fallback_translator.translate(original) or '').strip()\n",
    "                    except Exception:\n",
    "                        fallback = ''\n",
    "                    if fallback and fallback.lower() != original_clean.lower():\n",
    "                        translated_clean = fallback\n",
    "                # If still empty or identical, skip so it can be retried in a future run\n",
    "            if not translated_clean or translated_clean.lower() == original_clean.lower():\n",
    "                failed_candidates += 1\n",
    "                continue\n",
    "\n",
    "            translation_cache[original] = translated_clean\n",
    "            translated_unique += 1\n",
    "            translated_rows += int(freq_series.get(original, 1))\n",
    "            successes.append((original, translated_clean))\n",
    "\n",
    "        if successes:\n",
    "            with open(cache_path, 'a') as f:\n",
    "                for original, translated_clean in successes:\n",
    "                    f.write(json.dumps({'text': original, 'translation': translated_clean}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        if PAUSE_BETWEEN_BATCHES:\n",
    "            time.sleep(PAUSE_BETWEEN_BATCHES)\n",
    "\n",
    "    elapsed = time.time() - start_tr if translated_unique else 0.0\n",
    "    if translated_unique:\n",
    "        rate = translated_unique / elapsed if elapsed > 0 else float('inf')\n",
    "        est_coverage = translated_rows / max(1, total_non_en_rows)\n",
    "        print(f\"Translated {translated_unique} unique strings in {elapsed:.1f}s ({rate:.1f} strings/s)\")\n",
    "        print(f\"Estimated coverage of non-English rows this run: {est_coverage:.1%}\")\n",
    "        if failed_candidates:\n",
    "            print(f\"  {failed_candidates} strings returned identical text and will be retried on the next run.\")\n",
    "        if translated_unique < len(candidates):\n",
    "            print(\"  Note: not all non-English strings were translated this run. Re-run later to continue using the cache.\")\n",
    "\n",
    "    df_comments.loc[non_en_mask, 'translated_text'] = df_comments.loc[non_en_mask, 'comment_text'].map(translation_cache).fillna(df_comments.loc[non_en_mask, 'comment_text'])\n",
    "\n",
    "# Final report\n",
    "non_en_mask = (df_comments['detected_lang'] != 'en') & (df_comments['detected_lang'] != 'unknown')\n",
    "translated_fraction = (df_comments.loc[non_en_mask, 'translated_text'] != df_comments.loc[non_en_mask, 'comment_text']).mean() if non_en_mask.any() else 0.0\n",
    "print(\"\\n=== Translation Summary ===\")\n",
    "print(f\"Non-English rows: {int(non_en_mask.sum())}\")\n",
    "print(f\"Translated fraction: {translated_fraction:.3f}\")\n",
    "print(f\"Total cache size: {len(translation_cache)} translations\")\n",
    "print(f\"Cell runtime: {time.time() - overall_start:.1f}s (target < 300s)\")\n",
    "if DO_TRANSLATE and translated_fraction < 0.999:\n",
    "    print(\"Remaining non-English rows will translate on the next run; cached results prevent duplicates.\")\n",
    "print(\"\\nLanguage distribution:\")\n",
    "print(df_comments['detected_lang'].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac8a42dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Translation Summary ===\n",
      "Non-English rows: 85564\n",
      "Translated fraction: 0.110\n",
      "Total cache size: 80864 translations\n",
      "Cell runtime: 222.3s (target < 300s)\n",
      "Remaining non-English rows will translate on the next run; cached results prevent duplicates.\n",
      "\n",
      "Language distribution:\n",
      "detected_lang\n",
      "en    314430\n",
      "ja     12937\n",
      "zh      8259\n",
      "ar      7586\n",
      "es      7367\n",
      "ru      5779\n",
      "pt      4609\n",
      "ko      3045\n",
      "fr      2751\n",
      "tr      2594\n",
      "la      2358\n",
      "hi      1238\n",
      "km      1164\n",
      "mr      1153\n",
      "fa       993\n",
      "Name: count, dtype: int64\n",
      "399994\n"
     ]
    }
   ],
   "source": [
    "translated_fraction = (df_comments.loc[non_en_mask, 'translated_text'] != df_comments.loc[non_en_mask, 'comment_text']).mean() if non_en_mask.any() else 0.0\n",
    "print(\"\\n=== Translation Summary ===\")\n",
    "print(f\"Non-English rows: {int(non_en_mask.sum())}\")\n",
    "print(f\"Translated fraction: {translated_fraction:.3f}\")\n",
    "print(f\"Total cache size: {len(translation_cache)} translations\")\n",
    "print(f\"Cell runtime: {time.time() - overall_start:.1f}s (target < 300s)\")\n",
    "if DO_TRANSLATE and translated_fraction < 0.999:\n",
    "    print(\"Remaining non-English rows will translate on the next run; cached results prevent duplicates.\")\n",
    "print(\"\\nLanguage distribution:\")\n",
    "print(df_comments['detected_lang'].value_counts().head(15))\n",
    "print(len(df_comments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4254079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-English rows: 85564\n",
      "Translated fraction: 0.110\n",
      "\n",
      "Top detected languages:\n",
      " detected_lang\n",
      "en    314430\n",
      "ja     12937\n",
      "zh      8259\n",
      "ar      7586\n",
      "es      7367\n",
      "ru      5779\n",
      "pt      4609\n",
      "ko      3045\n",
      "fr      2751\n",
      "tr      2594\n",
      "la      2358\n",
      "hi      1238\n",
      "km      1164\n",
      "mr      1153\n",
      "fa       993\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "non_en = (df_comments['detected_lang'] != 'en') & (df_comments['detected_lang'] != 'unknown')\n",
    "coverage = (df_comments.loc[non_en, 'translated_text'] != df_comments.loc[non_en, 'comment_text']).mean()\n",
    "print(\"Non-English rows:\", int(non_en.sum()))\n",
    "print(\"Translated fraction:\", f\"{coverage:.3f}\")\n",
    "print(\"\\nTop detected languages:\\n\", df_comments['detected_lang'].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6738cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  Did Miley Cyrus and Liam Hemsworth secretly ge...      1\n",
      "1  Paris Jackson & Cara Delevingne Enjoy Night Ou...      1\n",
      "2  Celebrities Join Tax March in Protest of Donal...      1\n",
      "3  Cindy Crawford's daughter Kaia Gerber wears a ...      1\n",
      "4      Full List of 2018 Oscar Nominations – Variety      1\n"
     ]
    }
   ],
   "source": [
    "print(df_fnn.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58222e6a",
   "metadata": {},
   "source": [
    "## 4. Create Balanced Training Dataset\n",
    "\n",
    "Finally, we'll combine and balance our training data to prepare it for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efbc1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training Data: 21853\n",
      "Training dataset size before balancing: 21853\n",
      "Class distribution before balancing:\n",
      " label\n",
      "0    16530\n",
      "1     5323\n",
      "Name: count, dtype: int64\n",
      "Training dataset size after balancing: 10646\n",
      "Class distribution after balancing:\n",
      " label\n",
      "0    5323\n",
      "1    5323\n",
      "Name: count, dtype: int64\n",
      "Class distribution after balancing:\n",
      " label\n",
      "0    5323\n",
      "1    5323\n",
      "Name: count, dtype: int64\n",
      "Splits sizes:  15296 3279 3278\n",
      "\n",
      "Data saved to: ../data\n"
     ]
    }
   ],
   "source": [
    "# Combined LIAR datasets\n",
    "df_liar = pd.concat([df_liar_train, df_liar_test, df_liar_val], ignore_index=True)\n",
    "\n",
    "# Created binary labels (true/false) from LIAR's multi-class labels\n",
    "df_liar['binary_label'] = df_liar['label'].apply(\n",
    "    lambda x: 1 if str(x).strip().lower() in ['pants-fire', 'false', 'barely-true'] else 0\n",
    ")\n",
    "\n",
    "# Prepared LIAR portion\n",
    "liar_training = pd.DataFrame({\n",
    "    'text': df_liar['text'],\n",
    "    'label': df_liar['binary_label']\n",
    "}).dropna(subset=['text', 'label'])\n",
    "\n",
    "try:\n",
    "    fnn_training = df_fnn[['text', 'label']].copy()\n",
    "except NameError:\n",
    "    fnn_training = pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "# Combined all training data\n",
    "training_data = pd.concat([liar_training, fnn_training], ignore_index=True)\n",
    "\n",
    "\n",
    "# Dropped duplicates and rows with empty text\n",
    "training_data['text'] = training_data['text'].astype(str).str.strip()\n",
    "training_data = training_data[(training_data['text'] != '') & training_data['label'].isin([0, 1])]\n",
    "training_data = training_data.drop_duplicates(subset=['text', 'label'])\n",
    "\n",
    "print(\"Length of Training Data:\", len(training_data))\n",
    "\n",
    "MAX_TRAIN_ROWS = 100_000  \n",
    "if len(training_data) > MAX_TRAIN_ROWS:\n",
    "    training_data = training_data.sample(n=MAX_TRAIN_ROWS, random_state=42)\n",
    "\n",
    "print(\"Training dataset size before balancing:\", len(training_data))\n",
    "print(\"Class distribution before balancing:\\n\", training_data['label'].value_counts())\n",
    "\n",
    "# Balanced the dataset\n",
    "min_class_size = int(training_data['label'].value_counts().min())\n",
    "balanced_data = pd.concat([\n",
    "    training_data[training_data['label'] == 0].sample(min_class_size, random_state=42),\n",
    "    training_data[training_data['label'] == 1].sample(min_class_size, random_state=42)\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"Training dataset size after balancing:\", len(balanced_data))\n",
    "print(\"Class distribution after balancing:\\n\", balanced_data['label'].value_counts())\n",
    "\n",
    "# Created stratified train/val/test splits for model training\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(balanced_data, test_size=0.15, stratify=balanced_data['label'], random_state=42)\n",
    "train_df, test_df = train_test_split(training_data, test_size=0.15, stratify=training_data['label'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1765, stratify=train_df['label'], random_state=42)  # ~15% of original as val\n",
    "\n",
    "\n",
    "print(\"Class distribution after balancing:\\n\", balanced_data['label'].value_counts())\n",
    "print(\"Splits sizes: \", len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "# Saved processed data\n",
    "balanced_data.to_csv(os.path.join(data_dir, 'balanced_data.csv'), index=False)\n",
    "training_data.to_csv(os.path.join(data_dir, 'training_data.csv'), index=False)\n",
    "train_df.to_csv(os.path.join(data_dir, 'training_data_train.csv'), index=False)\n",
    "val_df.to_csv(os.path.join(data_dir, 'training_data_val.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(data_dir, 'training_data_test.csv'), index=False)\n",
    "\n",
    "df_comments.to_csv(os.path.join(data_dir, 'processed_comments.csv'), index=False)\n",
    "\n",
    "print(\"\\nData saved to:\", data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd1970c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-English rows: 85564\n",
      "Translated fraction: 0.110\n",
      "\n",
      "Top detected languages:\n",
      " detected_lang\n",
      "en    314430\n",
      "ja     12937\n",
      "zh      8259\n",
      "ar      7586\n",
      "es      7367\n",
      "ru      5779\n",
      "pt      4609\n",
      "ko      3045\n",
      "fr      2751\n",
      "tr      2594\n",
      "la      2358\n",
      "hi      1238\n",
      "km      1164\n",
      "mr      1153\n",
      "fa       993\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "non_en = (df_comments['detected_lang'] != 'en') & (df_comments['detected_lang'] != 'unknown')\n",
    "coverage = (df_comments.loc[non_en, 'translated_text'] != df_comments.loc[non_en, 'comment_text']).mean()\n",
    "print(\"Non-English rows:\", int(non_en.sum()))\n",
    "print(\"Translated fraction:\", f\"{coverage:.3f}\")\n",
    "print(\"\\nTop detected languages:\\n\", df_comments['detected_lang'].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc0fde5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untranslated (non-English) rows: 76116\n",
      "    detected_lang                                       comment_text\n",
      "11             zh             TRUMP MAGA MOVEMENT FOREVER ✝️🇺🇸🇵🇪🍊😎😁👍\n",
      "14             zh                                                 🖕🐷\n",
      "31             ko  도올선생님! 항상 건강 하셔서 국민을 향해 바른 소리를 계속 외쳐 주시기 바랍니다....\n",
      "39             ko  국민이 선택 했습니다 인정하고 지금도 반성없는 폐거리정치 선전선동만하는 북한정치를 ...\n",
      "53             ur                                         😭😭😌😎💩💩💩💩💩💩\n",
      "63             km                                                 💜💜\n",
      "64             ne                                              🔥🔥🔥🔥🔫\n",
      "87             ja                               最＆高です(*´︶`*)♡Thanks!\n",
      "127            uk  Ой бля дурак, тебе ж все равно жопа....руских ...\n",
      "143            km                                         💜💜💜💜💜💜💜💜💜💜\n"
     ]
    }
   ],
   "source": [
    "failed = df_comments[non_en & (df_comments['translated_text'] == df_comments['comment_text'])]\n",
    "print(\"Untranslated (non-English) rows:\", len(failed))\n",
    "print(failed[['detected_lang','comment_text']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c717138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated successfully for non-English rows: 9448\n",
      "   detected_lang                                       comment_text                                    translated_text\n",
      "6             tr  Keşke kiralara değinselerdi adam ne güzel dedi...  I wish they had mentioned the rents. The man s...\n",
      "7             ja                                        なぜ倒置法にしたんだ？              Why did you use the inversion method?\n",
      "20            es                             Dios bendiga esta niña                                God bless this girl\n",
      "26            zh                                             zango🎹                                             camp 🎹\n",
      "27            pt  Gosto muito dos vídeos de vcs. Só vcs mesmo pr...  I really like your videos. Only you to make a ...\n",
      "34            ja  上島さんのこの和やかな素敵な笑顔をもうテレビで見ることが出来ないのは少し寂しいですが、沢山の...  I'm a little sad that we won't be able to see ...\n",
      "36            ru  Помню как только вышла эта песня, я считал что...  I remember as soon as this song came out, I th...\n",
      "40            sl                                         Lo mejor 👍                                         The best 👍\n",
      "41            ja                    網戸越しに見る風景より開けて見たほうが綺麗だった＼(^o^)／  The view with the screen door open was more be...\n",
      "43            es                               Chagiya kamu hebat 😭                            Chagiya you are great 😭\n"
     ]
    }
   ],
   "source": [
    "passed = df_comments[non_en & (df_comments['translated_text'] != df_comments['comment_text'])]\n",
    "print(\"Translated successfully for non-English rows:\", len(passed))\n",
    "print(passed[['detected_lang','comment_text','translated_text']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "faf733b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated rows: 9,448 / 85,564 (11.04% success rate)\n"
     ]
    }
   ],
   "source": [
    "non_en = df_comments['detected_lang'].ne('en') & df_comments['detected_lang'].ne('unknown')\n",
    "translated_mask = df_comments.loc[non_en, 'translated_text'].ne(df_comments.loc[non_en, 'comment_text'])\n",
    "\n",
    "translated_rows = translated_mask.sum()\n",
    "total_non_en = non_en.sum()\n",
    "success_rate = translated_rows / total_non_en if total_non_en else 0.0\n",
    "\n",
    "print(f\"Translated rows: {translated_rows:,} / {total_non_en:,} \"\n",
    "      f\"({success_rate:.2%} success rate)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe978c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
